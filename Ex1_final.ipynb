{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bea4e301",
   "metadata": {},
   "source": [
    "# Pip installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffa2859",
   "metadata": {},
   "source": [
    "use pip to install the openCV software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ffdbb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed915b4",
   "metadata": {},
   "source": [
    "use pip to install the NumPy software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2394660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45be7b58",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ccf891",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8493893",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import cv2 \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53cd0f1",
   "metadata": {},
   "source": [
    "The class function accepts the path of the video and detect the moving object detection using frame differencing with OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76956357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function accept the path of the library and get the \n",
    "class Car_detector():\n",
    "    def __init__(self,filePath):\n",
    "        self.filepath = filePath\n",
    "    \n",
    "    def get_background(self):\n",
    "        #open video from a given path\n",
    "        video = cv2.VideoCapture(self.filepath)\n",
    "        # calculate the median with the 50 randaomly selected frames\n",
    "        frame_indices = video.get(cv2.CAP_PROP_FRAME_COUNT) * np.random.uniform(size=50)\n",
    "        # store the frames in array to store the frames\n",
    "        frames = []\n",
    "        \n",
    "        for i in frame_indices:\n",
    "            # set the frame id to read that particular frame\n",
    "            video.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            ret, frame = video.read()\n",
    "            frames.append(frame)\n",
    "            \n",
    "        # calculate the median with numpy median function\n",
    "        median_frame = np.median(frames, axis=0).astype(np.uint8)\n",
    "        return median_frame\n",
    "    \n",
    "    def frame_differencing(self):\n",
    "        # convert background to grayscale and let it be the base frame\n",
    "        background = cv2.cvtColor(self.get_background(),cv2.COLOR_BGR2GRAY)\n",
    "        cap = cv2.VideoCapture(self.filepath)\n",
    "        #height of the video frame to help in identifying the area of main street\n",
    "        height = int(cap.get(4))\n",
    "\n",
    "        while (cap.isOpened()):\n",
    "            ret,frame = cap.read()\n",
    "\n",
    "            if ret is True:\n",
    "                # convert frame to grayscale\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                # find the difference between current frame and base frame\n",
    "                frame_diff = cv2.absdiff(gray, background)\n",
    "                #thresholding to convert the frame to binary\n",
    "                ret,threshold_frame=cv2.threshold(frame_diff,50,255, cv2.THRESH_BINARY)\n",
    "                #dilate the frame to get some more white area\n",
    "                dilate_frame = cv2.dilate(threshold_frame,None,iterations=5)\n",
    "\n",
    "                frameCopy = frame.copy()\n",
    "                \n",
    "                #find the contours around the white segmented areas \n",
    "                contours, _ = cv2.findContours(dilate_frame,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "                for cnt in contours:\n",
    "                    # remove noise by continuing only is the contour area us less than 500\n",
    "                    if cv2.contourArea(cnt) < 500:\n",
    "                        continue\n",
    "                    # get the xmin, ymin, width, and height coordinates from the contours\n",
    "                    (x,y,w,h) = cv2.boundingRect(cnt)\n",
    "                    # draw the bouding boxes only if the car is found on the main street\n",
    "                    # and if the area is bigger than 6000\n",
    "                    if y > height/2 and w*h > 6000:\n",
    "                        cv2.rectangle(frameCopy,(x,y),(x +w,y+h),(0,255,0),2)\n",
    "\n",
    "\n",
    "                #cv2.imshow('MASK frame', erode)\n",
    "                cv2.imshow('FG MASK frame', frameCopy)\n",
    "\n",
    "\n",
    "                # Wait until a key is pressed.\n",
    "                # Retreive the ASCII code of the key pressed\n",
    "                k = cv2.waitKey(1) & 0xff\n",
    "\n",
    "                # Check if 'q' key is pressed.\n",
    "                if k == ord('q'):\n",
    "\n",
    "                    # Break the loop.\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Release the VideoCapture Object.\n",
    "        cap.release()\n",
    "\n",
    "        # Close the windows.q\n",
    "        cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0f1b95",
   "metadata": {},
   "source": [
    "In the Car_detector class function, two functions are defined which is get_background() and frame_differencing(). For the get_background function, the function is used to get the background model of a video where it will randomly select a few frames from the video file and calculate the median of those frames. The background model will be returned and it can be use for frame differencing. With the 50 randomly selected frame, the index position is set to the indices and the frames will be read. Those frames will be proceed to be stored in the frames list. The median frame will then be calculated from the frames list and return the background model found.\n",
    "\n",
    "For the frame_differencing function, it will be used to detect the moving object by comparing the frame difference between the background model and current frame. Video file is read with the cv2.VideoCapture function.  While the reading of video information is true, the frames in the video is loop over where we find the difference between the current frame and the background model. Both the background model and currrent frame are converted to grayscale color format as frame differencing between frames can only be done with the same colour state. Thresholding helps to convert the resulting frame to binary format. If the pixel value is less than 50, it will be set it 0 else it will be set to 255. Then, dilating the thresholding result where we expands the white pixel regions. 5 iterations is used so that the white patches of a car will be more obvious and it will reduce the probabilty of having multiple rectangles around the same moving cars. This helps in removing the noise from the resulting image. The contours of the moving object were then found, then go over the contours and if the contour area is less than 500 do not do anything. If not, extract the minimum x and y coordinates and the width and height for each contour. Then, draw rectangle on the area of the current contour if it is more than 6000 and if it is found on below half of the frame. This can help to filter out the noise such as passerby are not detected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b96e100",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector1 = Car_detector('Traffic_Laramie_1.mp4')\n",
    "detector1.frame_differencing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43dfea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector2 = Car_detector('Traffic_Laramie_2.mp4')\n",
    "detector2.frame_differencing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2eae82",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb754b0",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5bc8c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "from datetime import datetime\n",
    "import cv2 \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754799f0",
   "metadata": {},
   "source": [
    "The class function MovingBbox() is used to keep the information about the movements of the bounding boxes. The city_car_counter() function will be used to detect the moving cars with background subtraction and count the number of cars that go from the city's downtown to the city centre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "997b0a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep bounding boxes movement infomartion\n",
    "class MovingBbox:\n",
    "    def __init__(self, x, y, w, h):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "        #moving direction\n",
    "        self.dir = ''\n",
    "        #if the box is had been counted\n",
    "        self.counted = False\n",
    "        #number of frames the bounding box has been tracked along\n",
    "        self.numOfFrames = 0\n",
    "        \n",
    "def city_car_counter(filepath):\n",
    "    #open video from given path\n",
    "    video = cv2.VideoCapture(filepath)\n",
    "    #calculate the precise number of cars per seconds with fps\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    #initialize the knn background object\n",
    "    backgroundObject = cv2.createBackgroundSubtractorKNN(detectShadows = False)\n",
    "    # store bounding boxes from previous frame\n",
    "    prev_bbox = []\n",
    "    #number of cars per min going towards city centre\n",
    "    carsPerMin = 0\n",
    "    start_time = datetime.now()\n",
    "    # counter for number of cars\n",
    "    counter = 0\n",
    "    \n",
    "    while (video.isOpened()):\n",
    "        #read a new frame\n",
    "        ret,frame = video.read()\n",
    "        \n",
    "        # Check if frame is not read correctly.\n",
    "        if not ret:\n",
    "            # Break the loop.\n",
    "            break\n",
    "        # Apply the background object on the frame to get the segmented mask. \n",
    "        fgmask = backgroundObject.apply(frame)\n",
    "        # Apply some morphological operations to make sure you have a good mask\n",
    "        fgmask = cv2.dilate(fgmask, None, 18)\n",
    "        fgmask = cv2.erode(fgmask, None, 10)\n",
    "        \n",
    "        ret,threshold_frame=cv2.threshold(fgmask,100,255, cv2.THRESH_BINARY)\n",
    "        \n",
    "        #Detect contours in the frame.\n",
    "        contours,_=cv2.findContours(threshold_frame,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        #identify the bounding boxes in current frame\n",
    "        curr_bbox = []\n",
    "        \n",
    "        for cnt in contours:\n",
    "            (x,y,w,h) = cv2.boundingRect(cnt)\n",
    "            #filter out human shapes and any small contours\n",
    "            if (h/w >1.5) | (cv2.contourArea(cnt) < 500):\n",
    "                continue\n",
    "            if w*h > 6000:\n",
    "                # add the bounding boxes for cars to current frame bbox\n",
    "                curr_bbox.append(MovingBbox(x, y, w, h))\n",
    "        \n",
    "        #check if bboxes is found in prevBboxes\n",
    "        for cbox in curr_bbox:\n",
    "            exists = False\n",
    "            \n",
    "            for pbox in prev_bbox:\n",
    "                #distance between bounding boxes from previous to current frame\n",
    "                deltaX = cbox.x - pbox.x\n",
    "                deltaY = cbox.y - pbox.y\n",
    "\n",
    "                #difference in size from previous to current frame\n",
    "                deltaW = cbox.w - pbox.w\n",
    "                deltaH = cbox.h - pbox.h\n",
    "                \n",
    "                #determine whether the current bbox is too far from previous bbox\n",
    "                if (abs(deltaX) > 10) | (abs(deltaY) > 10) | (abs(deltaW) > 10) |  (abs(deltaH) > 10):\n",
    "                    continue\n",
    "                \n",
    "                exists = True\n",
    "                #find the direction on X\n",
    "                if deltaX > 0:\n",
    "                    cbox.dir = 'right'\n",
    "                elif deltaX < 0:\n",
    "                    cbox.dir = 'left'\n",
    "                    cbox.numOfFrames = pbox.numOfFrames + 1\n",
    "                else:\n",
    "                    cbox.dir = ''\n",
    "                    \n",
    "                #keep track of counted bboxed from frame to frame\n",
    "                cbox.counted = pbox.counted\n",
    "                \n",
    "                #if the car come from the right frame and exit from the left frame\n",
    "                if (cbox.x < 200) and (cbox.numOfFrames > 10) and (cbox.dir == 'left') and (cbox.counted == False):\n",
    "                    counter+=1\n",
    "                    cbox.counted = True\n",
    "            \n",
    "            # if previous bbox found, skip the remaining\n",
    "            if exists ==True:\n",
    "                continue\n",
    "                \n",
    "        #delta time from the beginning of the video up to the current frame\n",
    "        deltaTime = datetime.now() - start_time \n",
    "        #number of seconds from the beginning\n",
    "        secs = deltaTime.total_seconds()\n",
    "        #number of cars detected per minute\n",
    "        carsPerMin = counter * 60 / secs\n",
    "        \n",
    "        \n",
    "        #Draw information on screen\n",
    "        cv2.putText(frame, f'Count:{counter} in {int(secs)}s - ({carsPerMin:.2f} cars/min)', (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "    \n",
    "        #assign current Bboxes to previous\n",
    "        prev_bbox = curr_bbox\n",
    "\n",
    "        \n",
    "        #Show frames on screen\n",
    "        cv2.imshow('Clean Mask', frame)\n",
    "\n",
    "\n",
    "        # Wait until a key is pressed.\n",
    "        # Retreive the ASCII code of the key pressed\n",
    "        k = cv2.waitKey(1) & 0xff\n",
    "\n",
    "        # Check if 'q' key is pressed.\n",
    "        if k == ord('q'):\n",
    "\n",
    "            # Break the loop.\n",
    "            break\n",
    "    \n",
    "\n",
    "    # After the loop release the video object\n",
    "    video.release()\n",
    "\n",
    "    # Destroy all the windows\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return counter,carsPerMin\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d658c603",
   "metadata": {},
   "source": [
    "The background subtraction used is KNN where shadows will be detected which means it will be shown as grey pixel. Firstly, an object is created to signify the KNN algorithm we are using for background subtraction. After every frame is read, apply() function is used in every frame of the video to remove the background. Dilate and erode operation is used to isolate the individual elements and joining disparate elements in the frame after removing the foreground. Dilation will be performed 18 times while erosion will be performed 10 times. Thresholding operation will convert the video frame to a binary image where is a particular image is greater than 100 it will be assigned the value to 255 otherwise 0. FindContours function is used to find the contours in the video frame. An empty array to store the current boxes is defined, then for each contour we will use the boundingRect to extract the minimum x and y coordinates and the width and height for each contour. With this we then filter out the human shapes and the small contours found. The moving object which is the car found will be appended to the array that store the boxes identified in the current frame. The MovingBox class function is used here before appending it, so that the information of the moving object can be tracked.\n",
    "\n",
    "For each of the bounding boxes in that array it will check across the empty array that store the boxes in previous frame, if the boxes is found calculate the movement direction and store it if not it is a new bounding box then do not do anything. for the bounding boxes in prev array, we will find the difference in size and the distance between the two boxes so that we can make sure it is the same boxes. if the x-axis or the bounding boxes is decreasing means it is moving left else it is moving right. we will count the car if it exit the frame when the direction of the moving box is left, if the boxes have not been counted, the number of frame that the bounding box has been tracked along is more than 10 and the x axis of the moving box is lesser than 200. for the car per minute, it is used by the length of the video over the how many cars is found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f6a2561",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1_count,v1_carPerMin = city_car_counter('Traffic_Laramie_1.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5249f70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2_count,v2_carPerMin = city_car_counter('Traffic_Laramie_2.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a195e02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Total Number of Cars</th>\n",
       "      <th>Cars per Minute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Traffic_Laramie_1.mp4</td>\n",
       "      <td>6</td>\n",
       "      <td>3.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Traffic_Laramie_2.mp4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Total Number of Cars  Cars per Minute\n",
       "0  Traffic_Laramie_1.mp4                     6             3.47\n",
       "1  Traffic_Laramie_2.mp4                     4             3.85"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[\"Traffic_Laramie_1.mp4\", v1_count, round(v1_carPerMin,2)], [\"Traffic_Laramie_2.mp4\", v2_count,round(v2_carPerMin,2)]]\n",
    "pd.DataFrame(data, columns=[\"\", \"Total Number of Cars\", \"Cars per Minute\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
